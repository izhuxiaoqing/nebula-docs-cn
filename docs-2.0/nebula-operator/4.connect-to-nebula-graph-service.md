# 通过Nebular Operator连接Nebula Graph数据库

使用Nebula Operator创建Nebula Graph集群后，用户可在Nebula Graph集群内部访问Nebula Graph数据库，也可在集群外访问Nebula Graph数据库。

## 前提条件

使用Nebula Operator创建Nebula Graph集群。具体步骤参考[使用Kubectl部署Nebula Graph集群](3.deploy-nebula-graph-cluster/3.1create-cluster-with-kubectl.md)或者[使用Helm部署Nebula Graph集群](3.deploy-nebula-graph-cluster/3.2create-cluster-with-helm.md)。

## 在Nebula Graph集群内连接Nebula Graph数据库

当使用Nebula Operator创建Nebula Graph集群后，Nebula Operator会自动在同一命名空间下，创建名为`<cluster-name>-graphd-svc`、类型为`ClusterIP`的Service。通过该Service的IP和数据库的端口号，用户可连接Nebula Graph数据库。

1. 查看Service，命令如下：

  ```bash
  $ kubectl get service -l app.kubernetes.io/cluster=<nebula>  #<nebula>为变量值，请用实际集群名称替换。
  NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                          AGE
  nebula-graphd-svc          ClusterIP   10.98.213.34   <none>        9669/TCP,19669/TCP,19670/TCP                     23h
  nebula-metad-headless      ClusterIP   None           <none>        9559/TCP,19559/TCP,19560/TCP                     23h
  nebula-storaged-headless   ClusterIP   None           <none>        9779/TCP,19779/TCP,19780/TCP,9778/TCP            23h
  ```

  `ClusterIP`类型的Service只允许在集群内部访问容器应用。更多信息，请参考[ClusterIP](https://kubernetes.io/docs/concepts/services-networking/service/)。

2. 使用上述`<cluster-name>-graphd-svc` Service的IP连接Nebula Graph数据库：

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- <nebula_console_name> -addr <cluster_ip>  -port <service_port> -u <username> -p <password>
  ```

  示例：

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- nebula-console -addr 10.98.213.34  -port 9669 -u root -p vesoft
  ```

  - `--image`：为连接Nebula Graph的工具Nebula Console的镜像。
  - `<nebula-console>`：自定义的Pod名称。
  - `-addr`：连接Graphd服务的IP地址，即`ClusterIP`类型的Service IP地址。
  - `-port`：连接Graphd服务的端口。默认端口为9669。
  - `-u`：Nebula Graph账号的用户名。未启用身份认证时，可以使用任意已存在的用户名（默认为root）。
  - `-p`：用户名对应的密码。未启用身份认证时，密码可以填写任意字符。

  如果返回以下内容，说明成功连接数据库：

  ```bash
  If you don't see a command prompt, try pressing enter.

  (root@nebula) [(none)]>
  ```

用户还可以使用**完全限定域名（FQDN）**连接数据库，域名格式为`<cluster-name>-graphd.<cluster-namespace>.svc.<CLUSTER_DOMAIN>`：

```bash
kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- <nebula_console_name> -addr <cluster_name>-graphd-svc.default.svc.cluster.local -port <service_port> -u <username> -p <password>
```
`CLUSTER_DOMAIN`的默认值为`cluster.local`。

## 通过`NodePort`在Nebula Graph集群外部连接Nebula Graph数据库

用户可创建`NodePort`类型的Service，通过节点IP和暴露的节点端口，从集群外部访问集群内部的服务。用户也可以使用云厂商（例如Azure、AWS等）提供的负载均衡服务，设置Service的类型为`LoadBalancer`。

`NodePort`类型的Service通过标签选择器`spec.selector`将前端的请求转发到带有标签`app.kubernetes.io/cluster: <cluster-name>`、`app.kubernetes.io/component: graphd`的Graphd pod中。

操作步骤如下：

1. 创建名为`graphd-nodeport-service.yaml`的文件。YAML文件内容如下：

  ```yaml
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app.kubernetes.io/cluster: nebula
      app.kubernetes.io/component: graphd
      app.kubernetes.io/managed-by: nebula-operator
      app.kubernetes.io/name: nebula-graph
    name: nebula-graphd-svc-nodeport
    namespace: default
  spec:
    externalTrafficPolicy: Local
    ports:
    - name: thrift
      port: 9669
      protocol: TCP
      targetPort: 9669
    - name: http
      port: 19669
      protocol: TCP
      targetPort: 19669
    selector:
      app.kubernetes.io/cluster: nebula
      app.kubernetes.io/component: graphd
      app.kubernetes.io/managed-by: nebula-operator
      app.kubernetes.io/name: nebula-graph
    type: NodePort
  ```

  - Nebula Graph默认使用`9669`端口为客户端提供服务。`19669`为Graph服务端口号。
  - `targetPort`的值为映射至Pod的端口，可自定义。

2. 执行以下命令使Service服务在集群中生效。

  ```bash
  kubectl create -f graphd-nodeport-service.yaml
  ```

3. 查看Service中Nebula Graph映射至集群节点的端口。

  ```bash
  kubectl get services
  ```

  返回：

  ```bash
  NAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                          AGE
  nebula-graphd-svc              ClusterIP   10.98.213.34   <none>        9669/TCP,19669/TCP,19670/TCP                     23h
  nebula-graphd-svc-nodeport     NodePort    10.107.153.129 <none>        9669:32236/TCP,19669:31674/TCP,19670:31057/TCP   24h
  nebula-metad-headless          ClusterIP   None           <none>        9559/TCP,19559/TCP,19560/TCP                     23h
  nebula-storaged-headless       ClusterIP   None           <none>        9779/TCP,19779/TCP,19780/TCP,9778/TCP            23h
  ```

  NodePort类型的Service中，映射至集群节点的端口为`32236`。

4. 使用节点IP和上述映射的节点端口连接Nebula Graph。
  
  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- <nebula_console_name> -addr <node_ip> -port <node_port> -u <username> -p <password>
  ```

  示例如下：

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- nebula-console2 -addr 192.168.8.24 -port 32236 -u root -p vesoft
  If you don't see a command prompt, try pressing enter.

  (root@nebula) [(none)]>
  ```

  - `--image`：为连接Nebula Graph的工具Nebula Console的镜像。
  - `<nebula-console>`：自定义的Pod名称。本示例为`nebula-console2`。
  - `-addr`：Nebula Graph集群中任一节点IP地址。本示例为`192.168.8.24`。
  - `-port`：Nebula Graph映射至节点的端口。本示例为`32236`。
  - `-u`：Nebula Graph账号的用户名。未启用身份认证时，可以使用任意已存在的用户名（默认为root）。
  - `-p`：用户名对应的密码。未启用身份认证时，密码可以填写任意字符。


## 通过`Ingress`在Nebula Graph集群外部连接Nebula Graph数据库

Nginx Ingress是Kubernetes Ingress的一个实现。Nginx Ingress观察Kubernetes集群的Ingress资源，将Ingress规则生成Nginx配置，使Nginx能够转发第7层流量。

用户可以通过HostNetwork和DaemonSet组合的模式使用Nginx Ingress从集群外部连接Nebula Graph集群。

由于使用了HostNetwork，Nginx Ingress pods不能被安排在同一个节点上。为了避免监听端口冲突，可以事先选择一些节点并将其标记为边缘节点，专门用于部署Nginx Ingress。 然后，Nginx Ingress以DaemonSet模式部署在这些节点上。

由于Ingress不支持TCP或UDP服务，为此nginx-ingress-controller使用`--tcp-services-configmap`和`--udp-services-configmap`参数指向一个ConfigMap，该ConfigMap中的键指需要使用的外部端口，值指要公开的服务的格式，值的格式为`<命名空间/服务名称>：<服务端口>`。

例如指向名为`tcp-services`的ConfigMap的配置如下：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: tcp-services
  namespace: nginx-ingress
data:
  # update 
  9769: "default/nebula-graphd-svc:9669"
```

在配置了名为`tcp-services`的ConfigMap后，其配置中的端口需要在定义了Nginx Ingress的服务中公开。

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx-ingress
    component: default-backend
  name: nginx-ingress-proxy-tcp
  namespace: nginx-ingress
spec:
  ports:
    - name: proxied-tcp
      port: 9769
      protocol: TCP
      targetPort: 9669
  selector:
    app: nginx-ingress
    component: default-backend
  type: "ClusterIP"
```

- `port`的值`9769`指外部端口，用户可自行设置。
- `targetPort`的值`9669`指要连接的graphd服务的端口。

完整的示例及操作步骤如下：

1. 创建名为`nginx-ingress-daemonset-hostnetwork.yaml`的文件。YAML文件内容如下：

  ```yaml
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: nginx-ingress-controller
    namespace: nginx-ingress
  data:
    keep-alive-requests: "100"
    upstream-keepalive-connections: "200"
    max-worker-connections: "65536"
  ---
  apiVersion: v1
  kind: ConfigMap
  metadata:
    name: tcp-services
    namespace: nginx-ingress
  data:
    9769: "default/nebula-graphd-svc:9669"
  ---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
    namespace: nginx-ingress
  ---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress-backend
    namespace: nginx-ingress
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
  rules:
    - apiGroups:
        - ""
      resources:
        - configmaps
        - endpoints
        - nodes
        - pods
        - secrets
      verbs:
        - list
        - watch
    - apiGroups:
        - ""
      resources:
        - nodes
      verbs:
        - get
    - apiGroups:
        - ""
      resources:
        - services
      verbs:
        - get
        - list
        - update
        - watch
    - apiGroups:
        - extensions
        - "networking.k8s.io" # k8s 1.14+
      resources:
        - ingresses
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - ""
      resources:
        - events
      verbs:
        - create
        - patch
    - apiGroups:
        - extensions
        - "networking.k8s.io" # k8s 1.14+
      resources:
        - ingresses/status
      verbs:
        - update
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: nginx-ingress
  subjects:
    - kind: ServiceAccount
      name: nginx-ingress
      namespace: nginx-ingress
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
    namespace: nginx-ingress
  rules:
    - apiGroups:
        - ""
      resources:
        - namespaces
      verbs:
        - get
    - apiGroups:
        - ""
      resources:
        - configmaps
        - pods
        - secrets
        - endpoints
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - ""
      resources:
        - services
      verbs:
        - get
        - list
        - update
        - watch
    - apiGroups:
        - extensions
        - "networking.k8s.io" # k8s 1.14+
      resources:
        - ingresses
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - extensions
        - "networking.k8s.io" # k8s 1.14+
      resources:
        - ingresses/status
      verbs:
        - update
    - apiGroups:
        - ""
      resources:
        - configmaps
      resourceNames:
        - ingress-controller-leader-nginx
      verbs:
        - get
        - update
    - apiGroups:
        - ""
      resources:
        - configmaps
      verbs:
        - create
    - apiGroups:
        - ""
      resources:
        - endpoints
      verbs:
        - create
        - get
        - update
    - apiGroups:
        - ""
      resources:
        - events
      verbs:
        - create
        - patch
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    labels:
      app: nginx-ingress
    name: nginx-ingress
    namespace: nginx-ingress
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: nginx-ingress
  subjects:
    - kind: ServiceAccount
      name: nginx-ingress
      namespace: nginx-ingress
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: nginx-ingress
      component: controller
    name: nginx-ingress-controller-metrics
    namespace: nginx-ingress
  spec:
    ports:
      - name: metrics
        port: 9913
        targetPort: metrics
    selector:
      app: nginx-ingress
      component: controller
    type: "ClusterIP"
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: nginx-ingress
      component: default-backend
    name: nginx-ingress-default-backend
    namespace: nginx-ingress
  spec:
    ports:
      - name: http
        port: 80
        protocol: TCP
        targetPort: http
    selector:
      app: nginx-ingress
      component: default-backend
    type: "ClusterIP"
  ---
  apiVersion: v1
  kind: Service
  metadata:
    labels:
      app: nginx-ingress
      component: default-backend
    name: nginx-ingress-proxy-tcp
    namespace: nginx-ingress
  spec:
    ports:
      - name: proxied-tcp
        port: 9769
        protocol: TCP
        targetPort: 9669
    selector:
      app: nginx-ingress
      component: default-backend
    type: "ClusterIP"
  ---
  apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    labels:
      app: nginx-ingress
      component: controller
    name: nginx-ingress-controller
    namespace: nginx-ingress
  spec:
    selector:
      matchLabels:
        app: nginx-ingress
        component: controller
    template:
      metadata:
        labels:
          app: nginx-ingress
          component: controller
      spec:
        dnsPolicy: ClusterFirst
        initContainers:
          - name: setsysctl
            image: busybox
            securityContext:
              privileged: true
            command:
              - sh
              - -c
              - |
                sysctl -w net.core.somaxconn=65535
                sysctl -w net.ipv4.ip_local_port_range="1024 65535"
                sysctl -w net.ipv4.tcp_tw_reuse=1
                sysctl -w fs.file-max=1048576
        containers:
          - name: nginx-ingress-controller
            image: "ccr.ccs.tencentyun.com/mirrors/nginx-ingress-controller:v0.34.1"
            imagePullPolicy: IfNotPresent
            args:
              - /nginx-ingress-controller
              - --default-backend-service=$(POD_NAMESPACE)/nginx-ingress-default-backend
              - --election-id=ingress-controller-leader
              - --ingress-class=nginx
              - --configmap=$(POD_NAMESPACE)/nginx-ingress-controller
              - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            securityContext:
              capabilities:
                drop:
                  - ALL
                add:
                  - NET_BIND_SERVICE
              runAsUser: 101
              allowPrivilegeEscalation: true
            env:
              - name: POD_NAME
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.name
              - name: POD_NAMESPACE
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.namespace
            livenessProbe:
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 3
            ports:
              - name: http
                containerPort: 80
                protocol: TCP
              - name: proxied-tcp
                containerPort: 9769
                protocol: TCP
              - name: https
                containerPort: 443
                protocol: TCP
              - name: metrics
                containerPort: 10254
                protocol: TCP
            readinessProbe:
              httpGet:
                path: /healthz
                port: 10254
                scheme: HTTP
              initialDelaySeconds: 10
              periodSeconds: 10
              timeoutSeconds: 1
              successThreshold: 1
              failureThreshold: 3
        hostNetwork: true
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:  //为运行该DaemonSet的节点指定标签。
                    - key: nginx-ingress
                      operator: In
                      values:
                        - "true"
        serviceAccountName: nginx-ingress
        terminationGracePeriodSeconds: 60  
  ```

  !!! note
  
        上述YAML中的资源对象均使用`nginx-ingress`命名空间。用户可执行`kubectl create namesapce nginx-ingress`创建命名空间，或者自定义其他命名空间。

2. 为任一节点（本示例使用的节点名为`worker2`，IP为`192.168.8.160`）打上标签，以运行上述YAML文件中名为`nginx-ingress-controller`的DaemonSet。

  ```bash
  kubectl label node worker2 nginx-ingress=true
  ```

3. 执行以下命令使Nginx Ingress在集群中生效。

  ```bash
  kubectl create -f nginx-ingress-daemonset-hostnetwork.yaml
  ```

  返回：

  ```bash
  configmap/nginx-ingress-controller created
  configmap/tcp-services created
  serviceaccount/nginx-ingress created
  serviceaccount/nginx-ingress-backend created
  clusterrole.rbac.authorization.k8s.io/nginx-ingress created
  clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress created
  role.rbac.authorization.k8s.io/nginx-ingress created
  rolebinding.rbac.authorization.k8s.io/nginx-ingress created
  service/nginx-ingress-controller-metrics created
  service/nginx-ingress-default-backend created
  service/nginx-ingress-proxy-tcp created
  daemonset.apps/nginx-ingress-controller created
  ```

  成功部署Nginx Ingress后，由于Nginx Ingress中配置的网络类型为`hostNetwork`，因此用户可通过部署了Nginx Ingress的节点的IP（`192.168.8.160`）和外部端口（`9769`）访问Nebula Graph服务。

4. 执行以下命令部署连接Nebula Graph服务的Console并通过宿主机IP（本示例为`192.168.8.160`）和上述配置的外部端口访问Nebula Graph服务。

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- <nebula_console_name> -addr <host_ip> -port <external_port> -u <username> -p <password>
  ```

  示例：

  ```bash
  kubectl run -ti --image vesoft/nebula-console:{{console.branch}} --restart=Never -- nebula-console -addr 192.168.8.160 -port 9769 -u root -p vesoft
  ```

  - `--image`：为连接Nebula Graph的工具Nebula Console的镜像。
  - `<nebula-console>`：自定义的Pod名称。本示例为`nebula-console`。
  - `-addr`：部署Nginx Ingress的节点IP，本示例为`192.168.8.160`。
  - `-port`：外网访问使用的的端口。本示例设置为`9769`。
  - `-u`：Nebula Graph账号的用户名。未启用身份认证时，可以使用任意已存在的用户名（默认为root）。
  - `-p`：用户名对应的密码。未启用身份认证时，密码可以填写任意字符。

  如果返回以下内容，说明成功连接数据库：

  ```bash
  If you don't see a command prompt, try pressing enter.

  (root@nebula) [(none)]>
  ```